#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VVNews ç‹æ•å¥•æ–°é—»æœºå™¨äºº - äº‘ç«¯å®Œæ•´ç‰ˆæœ¬
åŠŸèƒ½: æ”¯æŒæ‰€æœ‰é¦™æ¸¯æ–°é—»æºï¼Œé€‚åˆäº‘ç«¯éƒ¨ç½²
"""

import requests
import feedparser
import re
import json
import os
import smtplib
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from bs4 import BeautifulSoup
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class VVNewsBotCloudComplete:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # é‚®ä»¶é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
        self.email_config = {
            'smtp_server': 'smtp.gmail.com',
            'smtp_port': 587,
            'sender_email': os.getenv('GMAIL_EMAIL', 'chingkeiwong666@gmail.com'),
            'sender_password': os.getenv('GMAIL_PASSWORD', 'scjrjhnfyohdigem'),
            'recipient_email': os.getenv('GMAIL_EMAIL', 'chingkeiwong666@gmail.com'),
            'subject_prefix': '[VVNews] ç‹æ•å¥•æœ€æ–°æ–°é—»'
        }
    
    def search_hk01(self, keyword):
        """æœç´¢é¦™æ¸¯01"""
        results = []
        
        try:
            logging.info(f"æœç´¢é¦™æ¸¯01: {keyword}")
            
            url = "https://www.hk01.com/zone/2/å¨›æ¨‚"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                content = response.text
                
                # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ–‡ç« 
                article_pattern = r'\{[^}]*"title":\s*"[^"]*' + keyword + r'[^"]*"[^}]*\}'
                matches = re.findall(article_pattern, content, re.IGNORECASE)
                
                for match in matches:
                    title_match = re.search(r'"title":\s*"([^"]*)"', match)
                    if title_match:
                        title = title_match.group(1)
                        
                        url_match = re.search(r'"canonicalUrl":\s*"([^"]*)"', match)
                        if url_match:
                            article_url = url_match.group(1)
                            if not article_url.startswith('http'):
                                article_url = 'https://www.hk01.com' + article_url
                        else:
                            id_match = re.search(r'"articleId":\s*(\d+)', match)
                            if id_match:
                                article_url = f"https://www.hk01.com/article/{id_match.group(1)}"
                            else:
                                continue
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'é¦™æ¸¯01',
                            'keyword': keyword
                        })
            
            logging.info(f"é¦™æ¸¯01 æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢é¦™æ¸¯01æ—¶å‡ºé”™: {e}")
            return results
    
    def search_google_news(self, keyword):
        """æœç´¢Google News"""
        results = []
        
        try:
            logging.info(f"æœç´¢Googleæ–°é—»: {keyword}")
            
            url = f"https://news.google.com/search?q={keyword}&hl=zh-TW&gl=HK&ceid=HK:zh-TW"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('article')
                
                for article in articles[:10]:  # é™åˆ¶ç»“æœæ•°é‡
                    title_elem = article.find('h3')
                    if title_elem and keyword.lower() in title_elem.get_text().lower():
                        title = title_elem.get_text().strip()
                        link_elem = article.find('a')
                        if link_elem:
                            article_url = 'https://news.google.com' + link_elem.get('href', '')
                            
                            results.append({
                                'title': title,
                                'url': article_url,
                                'source': 'Google News',
                                'keyword': keyword
                            })
            
            logging.info(f"Google News æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢Google Newsæ—¶å‡ºé”™: {e}")
            return results
    
    def search_youtube(self, keyword):
        """æœç´¢YouTube - RSSä¼˜å…ˆï¼Œæ”¯æŒå¤šé¢‘é“ä¸æ—¶é—´çª—å£"""
        results = []
        try:
            env_ids = os.getenv('YOUTUBE_CHANNEL_IDS', '').strip()
            if env_ids:
                raw_ids = [x.strip() for x in env_ids.split(',') if x.strip()]
            else:
                raw_ids = ['@TVBENews', '@TVB']

            channel_ids = []
            for ident in raw_ids:
                cid = self._resolve_youtube_channel_id(ident)
                if cid:
                    channel_ids.append(cid)

            if not channel_ids:
                logging.warning("æœªè§£æåˆ°æœ‰æ•ˆçš„YouTubeé¢‘é“IDï¼Œè·³è¿‡YouTubeæœç´¢")
                return results

            from datetime import datetime as _dt
            now_ts = _dt.utcnow()
            cutoff_hours = float(os.getenv('SEARCH_HOURS', '24'))
            max_per_channel = 5

            for channel_id in channel_ids:
                rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
                try:
                    feed = feedparser.parse(rss_url)
                except Exception as e:
                    logging.warning(f"è§£æRSSå¤±è´¥: {rss_url} -> {e}")
                    continue

                for entry in feed.entries[:max_per_channel]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    published_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
                    publish_time_iso = None
                    publish_time_readable = None
                    within_window = True
                    if published_parsed:
                        publish_dt = _dt(*published_parsed[:6])  # UTC
                        publish_time_iso = publish_dt.isoformat()
                        publish_time_readable = publish_dt.strftime('%Y-%m-%d %H:%M:%S')
                        diff_hours = (now_ts - publish_dt).total_seconds() / 3600.0
                        within_window = diff_hours <= float(cutoff_hours)

                    if keyword.lower() in title.lower() and within_window:
                        results.append({
                            'title': title,
                            'url': link,
                            'source': 'YouTube',
                            'publish_time': publish_time_iso,
                            'publish_time_readable': publish_time_readable,
                            'keyword': keyword
                        })

            logging.info(f"YouTube RSS æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
        except Exception as e:
            logging.error(f"æœç´¢YouTubeæ—¶å‡ºé”™: {e}")
            return results

    def _resolve_youtube_channel_id(self, handle_or_url: str) -> str:
        ident = handle_or_url.strip()
        try:
            if ident.startswith('UC') and len(ident) > 10:
                return ident
            if 'youtube.com/channel/' in ident:
                m = re.search(r"/channel/([A-Za-z0-9_-]+)", ident)
                return m.group(1) if m else ''
            if ident.startswith('http'):
                url = ident
            elif ident.startswith('@'):
                url = f"https://www.youtube.com/{ident}"
            else:
                url = f"https://www.youtube.com/@{ident}"

            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                m = re.search(r'"channelId":"(UC[^"]+)"', resp.text)
                if m:
                    return m.group(1)
        except Exception as e:
            logging.warning(f"è§£æé¢‘é“IDå¤±è´¥: {ident} -> {e}")
        return ''
    
    def search_oncc(self, keyword):
        """æœç´¢æ±ç¶²on.cc"""
        results = []
        
        try:
            logging.info(f"æœç´¢æ±ç¶²on.cc: {keyword}")
            
            url = "https://hk.on.cc/hk/entertainment/index.html"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://hk.on.cc' + article_url
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'æ±ç¶²on.cc',
                            'keyword': keyword
                        })
            
            logging.info(f"æ±ç¶²on.cc æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢æ±ç¶²on.ccæ—¶å‡ºé”™: {e}")
            return results
    
    def search_singtao(self, keyword):
        """æœç´¢æ˜Ÿå³¶å¨›æ¨‚"""
        results = []
        
        try:
            logging.info(f"æœç´¢æ˜Ÿå³¶å¨›æ¨‚: {keyword}")
            
            url = "https://www.stheadline.com/entertainment/"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://www.stheadline.com' + article_url
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'æ˜Ÿå³¶å¨›æ¨‚',
                            'keyword': keyword
                        })
            
            logging.info(f"æ˜Ÿå³¶å¨›æ¨‚ æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢æ˜Ÿå³¶å¨›æ¨‚æ—¶å‡ºé”™: {e}")
            return results
    
    def search_mingpao(self, keyword):
        """æœç´¢æ˜å ±"""
        results = []
        
        try:
            logging.info(f"æœç´¢æ˜å ±: {keyword}")
            
            url = "https://ol.mingpao.com/ldy/main.php"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://ol.mingpao.com' + article_url
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'æ˜å ±',
                            'keyword': keyword
                        })
            
            logging.info(f"æ˜å ± æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢æ˜å ±æ—¶å‡ºé”™: {e}")
            return results
    
    def search_mpweekly(self, keyword):
        """æœç´¢æ˜å‘¨"""
        results = []
        
        try:
            logging.info(f"æœç´¢æ˜å‘¨: {keyword}")
            
            url = "https://www.mpweekly.com/entertainment/"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://www.mpweekly.com' + article_url
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'æ˜å‘¨',
                            'keyword': keyword
                        })
            
            logging.info(f"æ˜å‘¨ æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢æ˜å‘¨æ—¶å‡ºé”™: {e}")
            return results
    
    def search_wenweipo(self, keyword):
        """æœç´¢é¦™æ¸¯æ–‡åŒ¯å ±"""
        results = []
        
        try:
            logging.info(f"æœç´¢é¦™æ¸¯æ–‡åŒ¯å ±: {keyword}")
            
            url = "https://www.wenweipo.com/"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://www.wenweipo.com' + article_url
                        
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'é¦™æ¸¯æ–‡åŒ¯å ±',
                            'keyword': keyword
                        })
            
            logging.info(f"é¦™æ¸¯æ–‡åŒ¯å ± æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢é¦™æ¸¯æ–‡åŒ¯å ±æ—¶å‡ºé”™: {e}")
            return results
    
    def search_tvb(self, keyword):
        """æœç´¢TVB - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«URLæ¨¡å¼åŒ¹é…å’Œç¼–ç å¤„ç†"""
        results = []
        
        try:
            logging.info(f"æœç´¢TVB: {keyword}")
            
            url = "https://www.tvb.com"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all('a', href=True)
                
                for article in articles:
                    title = article.get_text().strip()
                    if keyword in title:
                        article_url = article.get('href')
                        if not article_url.startswith('http'):
                            article_url = 'https://www.tvb.com' + article_url
                        
                        # å°è¯•æå–å‘å¸ƒæ—¶é—´
                        pub_iso, pub_readable = self._extract_tvb_publish_time(article_url)
                        results.append({
                            'title': title,
                            'url': article_url,
                            'source': 'TVB',
                            'keyword': keyword,
                            'publish_time': pub_iso,
                            'publish_time_readable': pub_readable
                        })
            
            logging.info(f"TVB æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢TVBæ—¶å‡ºé”™: {e}")
            return results
    
    def search_all_sources(self, keyword):
        """æœç´¢æ‰€æœ‰æ–°é—»æº"""
        all_results = []
        
        # æœç´¢å„ä¸ªæ¥æº
        all_results.extend(self.search_hk01(keyword))
        all_results.extend(self.search_google_news(keyword))
        all_results.extend(self.search_youtube(keyword))
        all_results.extend(self.search_oncc(keyword))
        all_results.extend(self.search_singtao(keyword))
        all_results.extend(self.search_mingpao(keyword))
        all_results.extend(self.search_mpweekly(keyword))
        all_results.extend(self.search_wenweipo(keyword))
        all_results.extend(self.search_tvb(keyword))
        
        return all_results

    def is_within_time_range(self, news_item, hours=None):
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…ï¼ŒTVB æ— å‘å¸ƒæ—¶é—´åˆ™æ’é™¤ã€‚"""
        try:
            from datetime import timedelta, timezone
            beijing_tz = timezone(timedelta(hours=8))
            current_time = datetime.now(beijing_tz)
            if hours is None:
                hours = float(os.getenv('SEARCH_HOURS', '24'))
            time_threshold = current_time - timedelta(hours=hours)

            publish_time = news_item.get('publish_time')
            if publish_time:
                news_time = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                if news_time.tzinfo is None:
                    news_time = news_time.replace(tzinfo=beijing_tz)
                else:
                    news_time = news_time.astimezone(beijing_tz)
                return news_time >= time_threshold

            # å¯¹ TVB æ›´ä¸¥æ ¼ï¼šæ²¡æœ‰æ—¶é—´å°±å°è¯•æå–ï¼Œå¤±è´¥åˆ™æ’é™¤
            url = (news_item.get('url') or '').lower()
            source = (news_item.get('source') or '').upper()
            if 'tvb.com' in url or source == 'TVB':
                pub_iso, _ = self._extract_tvb_publish_time(news_item.get('url') or '')
                if not pub_iso:
                    return False
                try:
                    news_time = datetime.fromisoformat(pub_iso.replace('Z', '+00:00'))
                    if news_time.tzinfo is None:
                        news_time = news_time.replace(tzinfo=beijing_tz)
                    else:
                        news_time = news_time.astimezone(beijing_tz)
                    return news_time >= time_threshold
                except Exception:
                    return False

            # å…¶ä»–æ¥æºä¿å®ˆæ”¾è¡Œ
            return True
        except Exception:
            return True

    def _extract_tvb_publish_time(self, article_url):
        """ä» TVB é¡µé¢æå–å‘å¸ƒæ—¶é—´ï¼Œè¿”å›(iso, readable)ï¼Œå¤±è´¥è¿”å›(None, None)ã€‚"""
        if not article_url:
            return None, None
        try:
            resp = self.session.get(article_url, timeout=6)
            if resp.status_code != 200:
                return None, None
            html = resp.text
            # å¸¸è§ meta/JSON-LD æ—¶é—´
            patterns = [
                r'<meta[^>]+property=["\"]article:published_time["\"][^>]+content=["\"]([^"\"]+)["\"]',
                r'<meta[^>]+itemprop=["\"]datePublished["\"][^>]+content=["\"]([^"\"]+)["\"]',
                r'<meta[^>]+name=["\"]pubdate["\"][^>]+content=["\"]([^"\"]+)["\"]',
                r'"datePublished"\s*:\s*"([^"]+)"',
                r'"uploadDate"\s*:\s*"([^"]+)"'
            ]
            from datetime import timezone, timedelta
            for pat in patterns:
                m = re.search(pat, html, re.IGNORECASE)
                if m:
                    raw = m.group(1).strip()
                    try:
                        dt = datetime.fromisoformat(raw.replace('Z', '+00:00'))
                        beijing_tz = timezone(timedelta(hours=8))
                        dt_bj = dt.astimezone(beijing_tz)
                        return dt.isoformat(), dt_bj.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception:
                        continue
            # å…œåº•ï¼šä¸­æ–‡/æ•°å­—æ—¥æœŸæ–‡æœ¬
            text_patterns = [
                r'(20\d{2})[-/](0?[1-9]|1[0-2])[-/](0?[1-9]|[12]\d|3[01])(\s+(\d{1,2}:\d{2})(:\d{2})?)?',
                r'(20\d{2})å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12]\d|3[01])æ—¥(\s+(\d{1,2}:\d{2})(:\d{2})?)?'
            ]
            for pat in text_patterns:
                m = re.search(pat, html)
                if m:
                    try:
                        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
                        hhmm = m.group(5) or '00:00'
                        from datetime import datetime as _dt
                        dt = _dt.strptime(f"{y:04d}-{mo:02d}-{d:02d} {hhmm}", '%Y-%m-%d %H:%M')
                        beijing_tz = timezone(timedelta(hours=8))
                        dt = dt.replace(tzinfo=beijing_tz)
                        return dt.isoformat(), dt.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception:
                        continue
        except Exception:
            return None, None
        return None, None
    
    def remove_duplicates(self, results):
        """å»é™¤é‡å¤ç»“æœ"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results
    
    def send_email(self, results):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        if not results:
            print("æ²¡æœ‰æ‰¾åˆ°æ–°é—»ï¼Œä¸å‘é€é‚®ä»¶")
            return False
        
        # æ£€æŸ¥é‚®ç®±é…ç½®
        if not self.email_config['sender_password']:
            print("é‚®ç®±å¯†ç æœªè®¾ç½®ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
            return False
        
        try:
            # åˆ›å»ºé‚®ä»¶å†…å®¹
            msg = MIMEMultipart()
            msg['From'] = self.email_config['sender_email']
            msg['To'] = self.email_config['recipient_email']
            msg['Subject'] = f"{self.email_config['subject_prefix']} - å‘ç° {len(results)} æ¡æ–°é—»"
            
            # æ„å»ºé‚®ä»¶æ­£æ–‡
            body = f"""
VVNews ç‹æ•å¥•æ–°é—»æœºå™¨äºº - äº‘ç«¯å®Œæ•´ç‰ˆæœ¬æ–°é—»æŠ¥å‘Š

æœç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ‰¾åˆ° {len(results)} æ¡æœ€æ–°æ–°é—»:

"""
            
            # æŒ‰æ¥æºåˆ†ç±»
            sources = {}
            for result in results:
                source = result.get('source', 'æœªçŸ¥')
                if source not in sources:
                    sources[source] = []
                sources[source].append(result)
            
            for source, source_results in sources.items():
                body += f"ğŸ“° {source} ({len(source_results)} æ¡):\n\n"
                for i, result in enumerate(source_results, 1):
                    body += f"{i}. {result['title']}\n"
                    body += f"   é“¾æ¥: {result['url']}\n\n"
            
            body += f"""
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- æ€»è®¡: {len(results)} æ¡
- æ¥æºæ•°é‡: {len(sources)} ä¸ª
- æœç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---
VVNews ç‹æ•å¥•æ–°é—»æœºå™¨äºº (äº‘ç«¯å®Œæ•´ç‰ˆæœ¬)
"""
            
            msg.attach(MIMEText(body, 'plain', 'utf-8'))
            
            # å‘é€é‚®ä»¶
            print("æ­£åœ¨å‘é€é‚®ä»¶é€šçŸ¥...")
            server = smtplib.SMTP(self.email_config['smtp_server'], self.email_config['smtp_port'])
            server.starttls()
            server.login(self.email_config['sender_email'], self.email_config['sender_password'])
            server.send_message(msg)
            server.quit()
            
            print(f"âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
            print(f"ğŸ“§ é‚®ä»¶å·²å‘é€åˆ°: {self.email_config['recipient_email']}")
            print(f"ğŸ“Š åŒ…å« {len(results)} æ¡æ–°é—»")
            return True
            
        except Exception as e:
            print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
            return False
    
    def save_results(self, results, filename=None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'vvnews_cloud_complete_{timestamp}.json'
        
        filepath = os.path.join('./results', filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logging.info(f"ç»“æœå·²ä¿å­˜åˆ° {filepath}")
            return filepath
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {str(e)}")
            return None
    
    def print_summary(self, results):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print(f"\nğŸ” æœç´¢å®Œæˆ:")
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ°: {len(results)} æ¡æ–°é—»")
        
        if results:
            # æŒ‰æ¥æºåˆ†ç±»
            sources = {}
            for result in results:
                source = result.get('source', 'æœªçŸ¥')
                if source not in sources:
                    sources[source] = []
                sources[source].append(result)
            
            print(f"\nğŸ“° æŒ‰æ¥æºåˆ†ç±»:")
            for source, source_results in sources.items():
                print(f"  {source}: {len(source_results)} æ¡")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–°é—»")
    
    def run(self, keyword='ç‹æ•å¥•'):
        """è¿è¡Œæœºå™¨äºº"""
        print(f"ğŸ¯ VVNews - ç‹æ•å¥•æ–°é—»æœºå™¨äºº (äº‘ç«¯å®Œæ•´ç‰ˆæœ¬)")
        print("=" * 60)
        print(f"å¼€å§‹æœç´¢å…³äº {keyword} çš„æ–°é—»...")
        print("=" * 60)
        
        # æœç´¢æ‰€æœ‰æ¥æº
        all_results = self.search_all_sources(keyword)
        
        # æ—¶é—´è¿‡æ»¤ + å»é‡
        filtered = [r for r in all_results if self.is_within_time_range(r, hours=float(os.getenv('SEARCH_HOURS','24')))]
        unique_results = self.remove_duplicates(filtered)
        
        # ä¿å­˜ç»“æœ
        if unique_results:
            self.save_results(unique_results)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        self.print_summary(unique_results)
        
        # å‘é€é‚®ä»¶
        if unique_results:
            print("\nğŸ“§ å‘é€é‚®ä»¶é€šçŸ¥...")
            self.send_email(unique_results)
        
        print(f"\nğŸ‰ VVNews äº‘ç«¯å®Œæ•´ç‰ˆæœ¬æœºå™¨äººè¿è¡Œå®Œæˆï¼")
        return unique_results

if __name__ == "__main__":
    bot = VVNewsBotCloudComplete()
    bot.run()
